{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7c857536-8c06-4cf2-9f0e-5c3e9377a94a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\SS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\SS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\SS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\SS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\SS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
      "[nltk_data]     C:\\Users\\SS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker_tab is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\SS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('maxent_ne_chunker_tab')  \n",
    "nltk.download('words')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c3802937-896e-4c8f-b2c0-bd145f3e92bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: The University of Faisalabad is a private institute, importing high quality education at level 1.\n",
      "\n",
      "1. Tokenization: ['The', 'University', 'of', 'Faisalabad', 'is', 'a', 'private', 'institute', ',', 'importing', 'high', 'quality', 'education', 'at', 'level', '1', '.']\n",
      "\n",
      "2. Lowercasing: ['the', 'university', 'of', 'faisalabad', 'is', 'a', 'private', 'institute', ',', 'importing', 'high', 'quality', 'education', 'at', 'level', '1', '.']\n",
      "\n",
      "3. Stop word removal: ['university', 'faisalabad', 'private', 'institute', 'importing', 'high', 'quality', 'education', 'level', '1']\n",
      "\n",
      "4a. Stemming: ['univers', 'faisalabad', 'privat', 'institut', 'import', 'high', 'qualiti', 'educ', 'level', '1']\n",
      "\n",
      "4b. Lemmatization: ['university', 'faisalabad', 'private', 'institute', 'importing', 'high', 'quality', 'education', 'level', '1']\n",
      "\n",
      "5. Remove digits/punctuation: ['university', 'faisalabad', 'private', 'institute', 'importing', 'high', 'quality', 'education', 'level']\n",
      "\n",
      "6. POS Tagging: [('university', 'NN'), ('faisalabad', 'NN'), ('private', 'JJ'), ('institute', 'NN'), ('importing', 'VBG'), ('high', 'JJ'), ('quality', 'NN'), ('education', 'NN'), ('level', 'NN')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\SS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\SS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\SS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\SS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\SS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\SS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "# Download resources (only first time)\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')   # ðŸ‘ˆ NEW fix\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Sample text\n",
    "text = \"The University of Faisalabad is a private institute, importing high quality education at level 1.\"\n",
    "\n",
    "print(\"Original Text:\", text)\n",
    "\n",
    "# 1. Tokenization\n",
    "tokens = word_tokenize(text)\n",
    "print(\"\\n1. Tokenization:\", tokens)\n",
    "\n",
    "# 2. Lowercasing\n",
    "lower_tokens = [w.lower() for w in tokens]\n",
    "print(\"\\n2. Lowercasing:\", lower_tokens)\n",
    "\n",
    "# 3. Stop word removal\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "filtered_tokens = [w for w in lower_tokens if w not in stop_words and w not in string.punctuation]\n",
    "print(\"\\n3. Stop word removal:\", filtered_tokens)\n",
    "\n",
    "# 4. Stemming & Lemmatization\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stems = [stemmer.stem(w) for w in filtered_tokens]\n",
    "lemmas = [lemmatizer.lemmatize(w) for w in filtered_tokens]\n",
    "print(\"\\n4a. Stemming:\", stems)\n",
    "print(\"\\n4b. Lemmatization:\", lemmas)\n",
    "\n",
    "# 5. Remove digits & punctuation\n",
    "clean_tokens = [w for w in lemmas if w.isalpha()]\n",
    "print(\"\\n5. Remove digits/punctuation:\", clean_tokens)\n",
    "\n",
    "# 6. POS Tagging\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "\n",
    "pos_tags = nltk.pos_tag(clean_tokens)\n",
    "print(\"\\n6. POS Tagging:\", pos_tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "463e4352-c86f-40e8-8876-a7607e008435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Tokens: ['Barack', 'Obama', 'was', 'born', 'in', 'Hawaii', '.', 'He', 'was', 'the', '44th', 'President', 'of', 'the', 'United', 'States', '.']\n",
      "\n",
      "2. Without stopwords: ['Barack', 'Obama', 'born', 'Hawaii', '.', '44th', 'President', 'United', 'States', '.']\n",
      "\n",
      "3. Stemming: ['barack', 'obama', 'born', 'hawaii', '.', '44th', 'presid', 'unit', 'state', '.']\n",
      "\n",
      "4. Lemmatization: ['barack', 'obama', 'born', 'hawaii', '.', '44th', 'president', 'united', 'state', '.']\n",
      "\n",
      "5. POS Tagging: [('Barack', 'NNP'), ('Obama', 'NNP'), ('born', 'VBD'), ('Hawaii', 'NNP'), ('.', '.'), ('44th', 'CD'), ('President', 'NNP'), ('United', 'NNP'), ('States', 'NNPS'), ('.', '.')]\n",
      "\n",
      "6. Named Entity Recognition (NER):\n",
      "PERSON â†’ Barack\n",
      "PERSON â†’ Obama\n",
      "PERSON â†’ Hawaii\n",
      "GPE â†’ United States\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "text = \"Barack Obama was born in Hawaii. He was the 44th President of the United States.\"\n",
    "\n",
    "# 1. Tokenization\n",
    "tokens = word_tokenize(text)\n",
    "print(\"1. Tokens:\", tokens)\n",
    "\n",
    "# 2. Stopword removal\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [w for w in tokens if w.lower() not in stop_words]\n",
    "print(\"\\n2. Without stopwords:\", filtered_tokens)\n",
    "\n",
    "# 3. Stemming\n",
    "stemmer = PorterStemmer()\n",
    "stems = [stemmer.stem(w) for w in filtered_tokens]\n",
    "print(\"\\n3. Stemming:\", stems)\n",
    "\n",
    "# 4. Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmas = [lemmatizer.lemmatize(w.lower()) for w in filtered_tokens]\n",
    "print(\"\\n4. Lemmatization:\", lemmas)\n",
    "\n",
    "# 5. POS Tagging\n",
    "pos_tags = nltk.pos_tag(filtered_tokens, lang='eng')\n",
    "print(\"\\n5. POS Tagging:\", pos_tags)\n",
    "\n",
    "# 6. Named Entity Recognition (NER)\n",
    "ner_tree = nltk.ne_chunk(pos_tags)\n",
    "\n",
    "print(\"\\n6. Named Entity Recognition (NER):\")\n",
    "for subtree in ner_tree:\n",
    "    if hasattr(subtree, 'label'):\n",
    "        print(f\"{subtree.label()} â†’ {' '.join(c[0] for c in subtree)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "30ddd5bf-57e6-4b62-9b34-3c2db171135a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:\n",
      " Natural Language Processing in 2025 is AMAZING!!! It deals with text, Speech, and Language understanding.\n",
      "\n",
      "1. Lowercased Text:\n",
      " natural language processing in 2025 is amazing!!! it deals with text, speech, and language understanding.\n",
      "\n",
      "2. Text without Punctuation/Numbers:\n",
      " natural language processing in  is amazing it deals with text speech and language understanding\n",
      "\n",
      "3. Tokens:\n",
      " ['natural', 'language', 'processing', 'in', 'is', 'amazing', 'it', 'deals', 'with', 'text', 'speech', 'and', 'language', 'understanding']\n",
      "\n",
      "4. Tokens after Stopword Removal:\n",
      " ['natural', 'language', 'processing', 'amazing', 'deals', 'text', 'speech', 'language', 'understanding']\n",
      "\n",
      "5. Stemming:\n",
      " ['natur', 'languag', 'process', 'amaz', 'deal', 'text', 'speech', 'languag', 'understand']\n",
      "\n",
      "6. Lemmatization:\n",
      " ['natural', 'language', 'processing', 'amazing', 'deal', 'text', 'speech', 'language', 'understanding']\n",
      "\n",
      "7. POS Tagging:\n",
      " [('natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('amazing', 'JJ'), ('deals', 'NNS'), ('text', 'VBP'), ('speech', 'JJ'), ('language', 'NN'), ('understanding', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "# Q.1 - Text Pre-Processing Program\n",
    "\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download necessary resources (run once)\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "# Sample raw text\n",
    "text = \"Natural Language Processing in 2025 is AMAZING!!! It deals with text, Speech, and Language understanding.\"\n",
    "\n",
    "print(\"Original Text:\\n\", text)\n",
    "\n",
    "# 1. Lowercasing\n",
    "text = text.lower()\n",
    "print(\"\\n1. Lowercased Text:\\n\", text)\n",
    "\n",
    "# 2. Remove punctuation and numbers\n",
    "text = ''.join([char for char in text if char not in string.punctuation and not char.isdigit()])\n",
    "print(\"\\n2. Text without Punctuation/Numbers:\\n\", text)\n",
    "\n",
    "# 3. Tokenization\n",
    "tokens = word_tokenize(text)\n",
    "print(\"\\n3. Tokens:\\n\", tokens)\n",
    "\n",
    "# 4. Stopword removal\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "print(\"\\n4. Tokens after Stopword Removal:\\n\", filtered_tokens)\n",
    "\n",
    "# 5. Stemming\n",
    "ps = PorterStemmer()\n",
    "stemmed = [ps.stem(word) for word in filtered_tokens]\n",
    "print(\"\\n5. Stemming:\\n\", stemmed)\n",
    "\n",
    "# 6. Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "print(\"\\n6. Lemmatization:\\n\", lemmatized)\n",
    "\n",
    "# 7. POS Tagging\n",
    "pos_tags = nltk.pos_tag(filtered_tokens)\n",
    "print(\"\\n7. POS Tagging:\\n\", pos_tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0ae7e74c-a3b2-4345-a01a-6d24f75d29c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4dd42e85-844c-43b4-b122-3a99b50375da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:\n",
      " Natural Language Processing in 2025 is AMAZING!!! It deals with text, Speech, and Language understanding.\n"
     ]
    }
   ],
   "source": [
    "text = \"Natural Language Processing in 2025 is AMAZING!!! It deals with text, Speech, and Language understanding.\"\n",
    "print(\"Original Text:\\n\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9ecab838-6f79-4097-8a07-b1b4eac7bb38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. Lowercased Text:\n",
      " natural language processing in 2025 is amazing!!! it deals with text, speech, and language understanding.\n"
     ]
    }
   ],
   "source": [
    "text = text.lower()\n",
    "print(\"\\n1. Lowercased Text:\\n\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4b0c2529-eda9-42e3-805c-258ac6d459a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2. Text without Punctuation/Numbers:\n",
      " natural language processing in  is amazing it deals with text speech and language understanding\n"
     ]
    }
   ],
   "source": [
    "text = ''.join([char for char in text if char not in string.punctuation and not char.isdigit()])\n",
    "print(\"\\n2. Text without Punctuation/Numbers:\\n\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "72c489c1-59d5-4919-a1ae-01099d8587bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3. Tokens:\n",
      " ['natural', 'language', 'processing', 'in', 'is', 'amazing', 'it', 'deals', 'with', 'text', 'speech', 'and', 'language', 'understanding']\n"
     ]
    }
   ],
   "source": [
    "tokens = word_tokenize(text)\n",
    "print(\"\\n3. Tokens:\\n\", tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ac4808ba-898a-47b3-82fe-476dfedb154c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4. Tokens after Stopword Removal:\n",
      " ['natural', 'language', 'processing', 'amazing', 'deals', 'text', 'speech', 'language', 'understanding']\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "print(\"\\n4. Tokens after Stopword Removal:\\n\", filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "466cc684-1ddf-4775-afd3-04c50833836d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5. Stemming:\n",
      " ['natur', 'languag', 'process', 'amaz', 'deal', 'text', 'speech', 'languag', 'understand']\n"
     ]
    }
   ],
   "source": [
    "ps = PorterStemmer()\n",
    "stemmed = [ps.stem(word) for word in filtered_tokens]\n",
    "print(\"\\n5. Stemming:\\n\", stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "26559505-d383-4dfc-9c28-b87350a8f2cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "6. Lemmatization:\n",
      " ['natural', 'language', 'processing', 'amazing', 'deal', 'text', 'speech', 'language', 'understanding']\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "print(\"\\n6. Lemmatization:\\n\", lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7697ab25-d89a-43bc-a8c1-eee61793ffbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "7. POS Tagging:\n",
      " [('natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('amazing', 'JJ'), ('deals', 'NNS'), ('text', 'VBP'), ('speech', 'JJ'), ('language', 'NN'), ('understanding', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "pos_tags = nltk.pos_tag(filtered_tokens)\n",
    "print(\"\\n7. POS Tagging:\\n\", pos_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b36fde1e-eeec-41b5-94fb-680cc9b6cd8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:\n",
      " The QUICK brown foxes were JUMPING over 123 lazy dogs, and it was AMAZING!!!\n",
      "\n",
      "Lowercased Text:\n",
      " the quick brown foxes were jumping over 123 lazy dogs, and it was amazing!!!\n",
      "\n",
      "Tokens:\n",
      " ['the', 'quick', 'brown', 'foxes', 'were', 'jumping', 'over', '123', 'lazy', 'dogs', ',', 'and', 'it', 'was', 'amazing', '!', '!', '!']\n",
      "\n",
      "After Removing Punctuation & Numbers:\n",
      " ['the', 'quick', 'brown', 'foxes', 'were', 'jumping', 'over', 'lazy', 'dogs', 'and', 'it', 'was', 'amazing']\n",
      "\n",
      "After Stopword Removal:\n",
      " ['quick', 'brown', 'foxes', 'jumping', 'lazy', 'dogs', 'amazing']\n",
      "\n",
      "After Stemming:\n",
      " ['quick', 'brown', 'fox', 'jump', 'lazi', 'dog', 'amaz']\n",
      "\n",
      "After Lemmatization:\n",
      " ['quick', 'brown', 'fox', 'jumping', 'lazy', 'dog', 'amazing']\n",
      "\n",
      "POS Tags:\n",
      " [('quick', 'JJ'), ('brown', 'NN'), ('foxes', 'NNS'), ('jumping', 'VBG'), ('lazy', 'JJ'), ('dogs', 'NNS'), ('amazing', 'VBG')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\SS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\SS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\SS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\SS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# Q.1: Text Preprocessing Program (Only NLTK)\n",
    "# -------------------------------\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "# Download resources (first time only)\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "# Raw text\n",
    "text = \"The QUICK brown foxes were JUMPING over 123 lazy dogs, and it was AMAZING!!!\"\n",
    "print(\"Original Text:\\n\", text)\n",
    "\n",
    "# Step 1: Lowercasing\n",
    "text = text.lower()\n",
    "print(\"\\nLowercased Text:\\n\", text)\n",
    "\n",
    "# Step 2: Tokenization\n",
    "tokens = nltk.word_tokenize(text)\n",
    "print(\"\\nTokens:\\n\", tokens)\n",
    "\n",
    "# Step 3: Remove punctuation and numbers\n",
    "tokens = [word for word in tokens if word.isalpha()]\n",
    "print(\"\\nAfter Removing Punctuation & Numbers:\\n\", tokens)\n",
    "\n",
    "# Step 4: Stopword removal\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "tokens = [word for word in tokens if word not in stop_words]\n",
    "print(\"\\nAfter Stopword Removal:\\n\", tokens)\n",
    "\n",
    "# Step 5: Stemming\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_tokens = [stemmer.stem(word) for word in tokens]\n",
    "print(\"\\nAfter Stemming:\\n\", stemmed_tokens)\n",
    "\n",
    "# Step 6: Lemmatization (using WordNetLemmatizer)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "print(\"\\nAfter Lemmatization:\\n\", lemmatized_tokens)\n",
    "\n",
    "# Step 7: POS tagging\n",
    "pos_tags = nltk.pos_tag(tokens)\n",
    "print(\"\\nPOS Tags:\\n\", pos_tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4065fc4-085c-4bc4-88e2-5376f0c0c08c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
